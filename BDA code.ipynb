{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    IMPORTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('AZ_cleaned.csv') #, index_col = 0) this line removes the index column but messes with other columns, see change log\n",
    "df #print(df.head(10)) for the time being, we can have it just as 'df' but I don't think that works in normal jupyter notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    IDENTIFYING COLUMN NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "for col in df:\n",
    "    column_names.append(col)\n",
    "\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    REMOVING ALL ROWS WHICH DO NOT CONTAIN VALUES IN THE 'STOP OUTCOME' OR 'DRIVER RACE' COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing any columns missing values, or having NaN in the 'stop_outcome' column\n",
    "df.dropna(how = 'any', subset = ['stop_outcome', 'driver_race'], axis = 0, inplace = True)\n",
    "df #removes ca. 200 rows, not much, but not doing this will result in problems in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.rename_axis('id').rename_axis('id', axis='columns') \n",
    "\"\"\" if having used the index_col = 0 function when importing, this sets the otherwise blank name \n",
    "of the 'id' column to be 'id' however the system does not actually read it's name as 'id'. \n",
    "No solution found thus far. \"\"\"\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    REMOVING ALL ROWS WHICH EITHER DO NOT CONTAIN ANY VALUES AT ALL, OR ARE THE RAW VERSIONS OF OTHER COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundancies(df_name):\n",
    "    redundant_columns = ['id','state', 'stop_date', 'location_raw', 'county_name', 'county_fips', 'fine_grained_location', \n",
    "    'driver_age_raw', 'driver_race_raw', 'violation_raw', 'search_type_raw', 'officer_id', 'stop_duration', 'road_number',\n",
    "    'milepost','vehicle_type', 'police_department', 'stop_time', 'driver_age'] #note: stop time is temporarily removed because the current code doesnt want to read it properly\n",
    "    df_name = df.drop(columns = redundant_columns, axis = 1, inplace = True)   #note 2: driver_age can be included if values are present in your dataset\n",
    "    return df_name\n",
    "\n",
    "remove_redundancies(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    BECAUSE WE SUSPECT A CORRELATION BEWTEEN DRIVER RACE, AND STOP OUTCOME, PLOTTING THE DRIVER RACE DISTRIBUTION TO SEE IF IT IS SKEWED TOWARDS CERTAIN GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.value_counts(df['driver_race']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "height = df.value_counts(df['driver_race'])\n",
    "bars = ('White','Hispanic','Other','Black','Asian')\n",
    "x_pos = np.arange(len(bars))\n",
    " \n",
    "plt.bar(x_pos, height, color = (0.2,0.7,0.3,0.9))\n",
    "\n",
    "plt.title('Distribution of driver race')\n",
    "plt.xlabel('Driver race')\n",
    "plt.ylabel('Instances (in millions)')\n",
    " \n",
    "plt.xticks(x_pos, bars)  #Create names on the x axis\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "\n",
    "X = df.drop(['stop_outcome'], axis = 1)\n",
    "y = df['stop_outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "Xd = pd.get_dummies(X, columns = ['driver_gender', 'driver_race','violation', 'search_conducted', 'search_type', \n",
    "                                  'contraband_found', 'is_arrested', 'consent_search', 'ethnicity'])\n",
    "print(Xd.shape[1] - X.shape[1], 'columns added') \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CREATING TESTING DATAFRAME USING FIRST 100000 VALUES TO DECREASE PROCESSING TIME WHILE TESTING CODE AND PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = df.head(100000)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "\n",
    "X = df_testing.drop(['stop_outcome'], axis = 1)\n",
    "y = df_testing['stop_outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "Xd = pd.get_dummies(X, columns = ['driver_gender', 'driver_race','violation', 'search_conducted', 'search_type', \n",
    "                                  'contraband_found', 'is_arrested', 'consent_search', 'ethnicity'])\n",
    "print(Xd.shape[1] - X.shape[1], 'columns added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd.dropna(axis = 0, inplace = True)\n",
    "Xd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    HYPERPARAMETER TUNING (TESTING ALL COMBINATIONS OF DIFFERENT PARAMETER VALUES, TO SEE WHAT GIVES THE BEST TRAIN AND TEST RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn import metrics\n",
    "%matplotlib inline  \n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "knn_model = KNeighborsClassifier()\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "\n",
    "def rforest_tuning():\n",
    "    Trees = [1, 10, 25, 50, 100, 150] #number of trees considered in random forest\n",
    "    maxFeatures = [1, 5, 10, 15] #number of features considered at each split\n",
    "    maxDepth = [1, 5, 10, 25, 50, 100] #max number of levels in a tree\n",
    "    minSamplesSplit = [1.0, 2, 3, 4] #minimum number of samples for a node to split\n",
    "    minSamplesLeaf = [1, 2, 3] #minimum number of samples required at each leaf node\n",
    "\n",
    "    parameter_grid = {'n_estimators' : Trees,\n",
    "                    'max_features' : maxFeatures,\n",
    "                    'max_depth' : maxDepth,\n",
    "                    'min_samples_split' : minSamplesSplit,\n",
    "                    'min_samples_leaf' : minSamplesLeaf}\n",
    "    \n",
    "    rf_grid = GridSearchCV(estimator = rf_model, param_grid = parameter_grid, cv = 3, verbose = 2, n_jobs = 4) \n",
    "    rf_grid.fit(X_train, y_train)\n",
    "\n",
    "    print(rf_grid.best_params_)\n",
    "    print (f'Train Accuracy : {rf_grid.score(X_train,y_train):.3f}')\n",
    "    print (f'Test Accuracy : {rf_grid.score(X_test,y_test):.3f}')\n",
    "\n",
    "    # identifying and plotting feature importance\n",
    "    importance = rf_model.feature_importances_\n",
    "    for i,j in enumerate(importance):\n",
    "        print(f'Feature: {i}, Score: {j}%.5f')\n",
    "\n",
    "    plt.bar([x for x in range(len(importance))], importance)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def knn_tuning():\n",
    "    parameter_grid = {'n_neighbors' : [1, 3, 5, 7, 9, 11, 13, 15],\n",
    "                      'weights' : ['uniform', 'distance'],\n",
    "                      'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "    knn_grid = GridSearchCV(estimator = knn_model, param_grid = parameter_grid, cv = 3, verbose = 2, n_jobs = -1)\n",
    "    knn_grid.fit(X_train, y_train)\n",
    "\n",
    "    print(knn_grid.best_params_)\n",
    "    print (f'Train Accuracy : {knn_grid.score(X_train,y_train):.3f}')\n",
    "    print (f'Test Accuracy : {knn_grid.score(X_test,y_test):.3f}')\n",
    "\n",
    "\n",
    "def GBM_tuning():\n",
    "    parameter_grid = {'min_samples_split': range(200, 1001, 200),\n",
    "                      'min_samples_leaf' : range(30, 71, 10),\n",
    "                      'max_depth' : range(5, 16, 2),\n",
    "                      'max_features' : ['sqrt'],\n",
    "                      'subsample' : [0.8]}\n",
    "    GBM_grid = GridSearchCV(estimator = gbm_model, param_grid = parameter_grid, cv = 3, verbose = 2, n_jobs = 4)\n",
    "    GBM_grid.fit(X_train, y_train)\n",
    "\n",
    "    print(GBM_grid.best_params_)\n",
    "    print (f'Train Accuracy : {GBM_grid.score(X_train,y_train):.3f}')\n",
    "    print (f'Test Accuracy : {GBM_grid.score(X_test,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)\n",
    "importance = rf_model.feature_importances_\n",
    "for i,j in enumerate(importance):\n",
    "    print(f'Feature: {i}, Score: {j:.5f}')\n",
    "\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(Xd, y)\n",
    "feature_importances=pd.DataFrame({'features':X.columns,'feature_importance':rf_model.feature_importances_})\n",
    "feature_importances.sort_values('feature_importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBM_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf_grid = GridSearchCV(estimator = rf_model, param_grid = parameter_grid, cv = 3, verbose = 2, n_jobs = 4) \n",
    "#rf_grid.fit(X_train, y_train) #the 'cv' value is the class validation score, by increasing it, it evalues the model n amount of times, can\n",
    "                              #improve accuracy, however it does make it take longer to run. With a cv of 10, the accuracy of test is 0.851."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_grid.best_params_)\n",
    "print (f'Train Accuracy : {rf_grid.score(X_train,y_train):.3f}')\n",
    "print (f'Test Accuracy : {rf_grid.score(X_test,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                THE CODE BELOW IS THE FIRST VERSIONS OF BOTH THE PARAMETER OPTIMIZATION AND FEATURE IMPORTANCE GRAPHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xd, y, random_state = 0)\n",
    "\n",
    "rforest = RandomForestClassifier(max_depth = 50, random_state = 0) #possible parameters :max_depth, min_sample_split, max_leaf_nodes\n",
    "                                                            #min_samples_leaf, n_estimators, max_sample (bootstrap sample), max_features\n",
    "\n",
    "rforest.fit(X_train, y_train)\n",
    "train_score = rforest.score(X_train, y_train)\n",
    "test_score = rforest.score(X_test, y_test)\n",
    "\n",
    "print('Train   Test')\n",
    "print('{:.3f} {:7.3f}'.format(train_score, test_score)) #unsure if these results are correct, is there a way to make it faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def specific_parameters():\n",
    "        user_inputs = []\n",
    "        parameters = ['max_depth', 'min_sample_split', 'max_leaf_nodes', 'min_samples_leaf', 'n_estimators', 'max_sample', 'max_features']\n",
    "        for item in parameters:\n",
    "            user_input = input(f'Please select a value for the parameter {item}:')\n",
    "            if user_input.type() != 'float':\n",
    "                user_inputs.append('None')\n",
    "            else:\n",
    "                user_inputs.append(user_input)\n",
    "        return user_inputs\n",
    "\n",
    "def test_parameter():\n",
    "    number = [1, 5, 10, 25, 50, 100]\n",
    "    print('Parameter size   Train   Test')\n",
    "    results_train = []\n",
    "    results_test = []\n",
    "    for i in number:\n",
    "        model = KNeighborsClassifier(n_neighbors = i, algorithm = 'auto', weights = 'distance')#max_features = 10, min_samples_leaf = 1, min_samples_split= 3, n_estimators= 150, random_state = 0)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        #TRAIN\n",
    "        train_score = model.score(X_train, y_train)\n",
    "        results_train.append(train_score)\n",
    "        \n",
    "        #TEST\n",
    "        test_score = model.score(X_test, y_test)\n",
    "        results_test.append(test_score)\n",
    "\n",
    "        print('{:8d} {:13.3f} {:7.3f}'.format(i, train_score, test_score))\n",
    "\n",
    "    x = number\n",
    "    y = results_train\n",
    "    plt.plot(x,y)\n",
    "    y = results_test \n",
    "    plt.plot(x,y)\n",
    "\n",
    "#test_parameter() #note --> random grid can be used to optimize finding the best feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(algorithm, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #fit the data to the algorithm\n",
    "    algorithm.fit(dtrain[predictors], dtrain['Disbursed'])\n",
    "    \n",
    "    #predict training set\n",
    "    dtrain_predictions = algorithm.predict(dtrain[predictors])\n",
    "    dtrain_probabilities = algorithm.predict_proba(dtrain[predictors])[:,1]\n",
    "\n",
    "    #perform cross validation\n",
    "    if performCV:\n",
    "        cv_score = model_selection.cross_val_score(algorithm, dtrain[predictors], dtrain['Disbursed'], cv = cv_folds, scoring = 'roc_auc')\n",
    "    \n",
    "    #model results\n",
    "    print('\\nModel Results')\n",
    "    print('Accuracy : %4g' %metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print('AUC score (train): %f' %metrics.roc_auc_score(dtrain['Disbursed'], dtrain_probabilities))\n",
    "\n",
    "    if performCV:\n",
    "        print('CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g' % (np.mean(cv_score), np.std(cv_score), np.min(cv_score), np.max(cv_score)))\n",
    "    \n",
    "    #feature importance\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(algorithm.feature_importances_, predictors).sort_values(ascending = False)\n",
    "        feat_imp.plot(kind = 'bar', title = 'Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
