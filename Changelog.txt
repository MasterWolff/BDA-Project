24/03/2022: (Klaus) 
    Updated remove_redundancies(df_name) 
        defined df_new as dropping the columns from the dataframe defined in the parameter "df_name". 
        The columns dropped are defined in the list redundant_columns.

24/03/2022: (Mads)
    Updated remove_redundancies(df_name)
        Though it looked to be working in Klaus' iteration, the result of the function was not being correctly saved
        and calling upon 'df' later would just call on the imported version, not the processed one.

    Started removing index column <-- Work in Progress
        When using the pd.read_csv() function, a new column is created that cannot be called upon, this is the index column
        and is what I am trying to remove. Though it is not directly influencing our data or the way we wish to progress 
        with our machine learning, it is just quite cluttered to look at because we have so many rows. I've not yet figured
        out how to make it work correctly, I can remove the index column during import, but then the 'id' column takes its
        place and creates a whole new slew of problems. for now they are muted as they influence the other code, feel free
        to fix if you want, I will try to fix at a later date if not resolved by then.

    Started importing for ML + TTS
        Imported train test split (TTS) and performed this on the data, considering the target value to be the
        'stop_outcome' column, and the data to be the rest.

27/03/2022: (Mads)
    Get dummies
        Created dummy values for all of the columns which contained non-numerical values, spoiler alert it was all of them.
        In total, 523 columns were added.

    Remove NaN values 
        Removed any NaN values present in the 'stop_outcome' column as the target value is always needed in order to create
        a prediction. The same was done for Xd (Xdummy) to remove any NaN values, but none were present.

    Created random forest classifier
        To test the getdummies, I created a basic random forest classifier and only tested it once, reason being that it
        took almost 34 minutes to do so, and returned the same values for train and test, so whether or not they are correct
        is also unlikely. Perhaps someone knows of a way to optimize this? maybe our professor if not one of us?

28/03/2022: (Mads)
    Dataset downsize for testing
        created a copy of the dataframe using the first 100000 rows, this referred to as 'df_testing' reduces run time of analysis by like 1000000%
    Testing parameters 
        testing different values for the max_depth parameter in the random forest, see test_parameter function
    WIP: Test parameters function
        began creating a version where the parameters you wish to test and the values with which you want to test them can be selected, currently not working, might
        also not be necessary, just makes it easier to test them.

30/03/2022 (Mads)
    Visualizing driver race distribution

31/03/2022 (Mads)
    Improving data visualisation, that's it.
        